{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Ajay Ugale\n",
    "#### Student ID: S3940207\n",
    "\n",
    "Date: 02/10/2022\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "\n",
    "## Introduction\n",
    "You should give a brief information of this assessment task here.\n",
    "\n",
    "<span style=\"color: red\"> Note that this is a sample notebook only. You will need to fill in the proper markdown and code blocks. You might also want to make necessary changes to the structure to meet your own needs. Note also that any generic comments written in this notebook are to be removed and replace with your own words.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "- xamine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The categories of the job adverstisement are: \n",
      "Accounting_Finance\n",
      "Engineering\n",
      "Healthcare_Nursing\n",
      "Sales\n",
      "Job description file count for the category Accounting_Finance is 191\n",
      "Job description file count for the category Engineering is 231\n",
      "Job description file count for the category Healthcare_Nursing is 198\n",
      "Job description file count for the category Sales is 156\n"
     ]
    }
   ],
   "source": [
    "# Identify the categories of job advertisement\n",
    "\n",
    "print(\"The categories of the job adverstisement are: \")\n",
    "for branch in [branches for branches in os.listdir('./data') if branches!='.DS_Store']:\n",
    "    print(branch)\n",
    "\n",
    "# Job advertisemetn file count for each category\n",
    "for branch in [branches for branches in os.listdir('./data') if branches!='.DS_Store']:\n",
    "    jd = os.listdir(f'./data/{branch}')\n",
    "    print(f\"Job description file count for the category {branch} is {len(jd)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... Sections and code blocks on basic text pre-processing\n",
    "\n",
    "\n",
    "<span style=\"color: red\"> You might have complex notebook structure in this section, please feel free to create your own notebook structure. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_text = []\n",
    "for branch in [branches for branches in os.listdir('./data') if branches!='.DS_Store']:\n",
    "    for jd in os.listdir(f'./data/{branch}'):\n",
    "        with open(os.path.join('./data', branch, jd), \"r\", encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            # Fetch the job description from the file\n",
    "            text = text.split('Description: ', 1)[1]            \n",
    "            # Regular expression for tokenization of each job description\n",
    "            reg = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "            # Remove words that have length less than 2\n",
    "            filtered = [word for word in re.findall(reg, text.lower()) if len(word)>=2]\n",
    "            with open(\"stopwords_en.txt\", \"r\") as f:\n",
    "              stopwords = f.readlines()\n",
    "            # Remove the stopwords \n",
    "            stopwords = [line.replace('\\n', '') for line in stopwords]\n",
    "            filtered = [word for word in filtered if word not in stopwords]\n",
    "            # Remove words having frequency 1\n",
    "            counts = Counter(filtered)\n",
    "            filtered = [word for word in filtered if counts[word]!=1]\n",
    "            global_text.extend(filtered)\n",
    "\n",
    "global_count = Counter(global_text)\n",
    "# Remove the top 50 most repeated words from the document\n",
    "top_50_freq = [word for word, count in global_count.most_common()[:50]]\n",
    "global_text = [word for word in global_text if word not in top_50_freq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "Save the vocabulary, bigrams and job advertisment txt as per spectification.\n",
    "- vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include unique values with their index\n",
    "global_text = np.array(global_text)\n",
    "words = np.unique(global_text)\n",
    "\n",
    "# Include index starting from 0 \n",
    "output = []\n",
    "for idx, word in enumerate(words):\n",
    "    output.append(f\"{word}:{idx}\")\n",
    "\n",
    "output = \"\\n\".join(output)\n",
    "\n",
    "# Save to disk\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here, we explored the files to see the file structure. We preprocessed the job description by tokenizing with regular expression, lower case, removing words having length less than 2, removing the stopwords present, removing words having frequency 1 in each document and removing the top 50 frequent words from all the documents. Then we created a vocabulary and saved it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
